# Question Answering System with PDFs

This project implements a question answering system using Hugging Face's RAG (Retrieve and Generate) model, integrated PDF processing, and a Streamlit web application for user interaction.

## Directory Structure

- `config/`: Configuration files for the project.
  - `config.yaml`: Main configuration file for the project.
  - `hf_login.py`: Contains functions for Hugging Face authentication. Replace `YOUR_HF_TOKEN` with your actual Hugging Face token.
  - `logging.yaml`: Configuration for logging.
- `data/`: Contains data files.
  - `pdfs/`: PDF files for processing.
    - `ConceptsofBiology1_new.pdf`: Chapter 1.
    - `ConceptsofBiology2_new.pdf`: Chapter 2.
  - `processed/`: Directory for processed data.
- `demo/`: Contains screenshots of the app.
  - `QnA1.png`: Screenshot of a question and answer interaction (Chapter 1).
  - `QnA2.png`: Screenshot of a question and answer interaction (Chapter 2).
  - `QnA_home_page.png`: Screenshot of the home page of the app.
  - `QnA.png`: Screenshot of the app with explanation.
- `Dockerfile`: Docker configuration file to build a containerized environment.
- `models/`: Directory for storing pre-trained models.
  - `pre-trained/`: Pre-trained models.
- `requirements.txt`: List of dependencies.
- `setup.py`: Setup script for the project.
- `src/`: Contains source code modules.
  - `embeddings.py`: Functions for loading Hugging Face embeddings.
  - `index_creator.py`: Functions for creating the index.
  - `__init__.py`: Initialization module for the `src` package.
  - `llm.py`: Functions for initializing the language model.
  - `pdf_loader.py`: Functions for loading PDF files.
  - `qa_chain.py`: Functions for creating the QA chain.
  - `ragas_evaluation.py`: Functions for evaluating the RAG model.
  - `streamlit_app.py`: Streamlit application code.
  - `text_utils.py`: Utility functions for text processing.
- `start.sh`: Shell script to start the Streamlit application.

## Installation

1. Install the dependencies:

    ```bash
    pip install -r requirements.txt
    ```

2. **(Optional) Docker Setup**

   If you prefer to run the project in a Docker container, build the Docker image with:

    ```bash
    sudo docker build -t question-answering-app .
    ```

   Then, run the Docker container with:

    ```bash
    sudo docker run -p 8501:8501 question-answering-app
    ```

   This will start the application and expose it on port 8501.

## Configuration

1. **Hugging Face Token**

   To use the Hugging Face API, provide your token in the `hf_login.py` file located in the `config/` directory. Open `hf_login.py` and replace `YOUR_HF_TOKEN` with your actual Hugging Face token:

    ```python
    HF_TOKEN = "YOUR_HF_TOKEN"
    ```

   Ensure you have a valid token from [Hugging Face](https://huggingface.co).

## Starting the Application

1. **Without Docker**

   To start the Streamlit application directly on your local machine, use the `start.sh` script:

    ```bash
    ./start.sh
    ```

   Make sure the script is executable. You can set the executable permission with:

    ```bash
    chmod +x start.sh
    ```

2. **With Docker**

   If youâ€™re using Docker, the application will start automatically after running the container. Access it by navigating to `http://localhost:8501` in your web browser.

## Usage

1. Open your web browser and navigate to `http://localhost:8501`.
2. Enter questions related to the content of the PDFs.
3. Get answers generated by the system.

## Demo

Refer to the `demo/` directory for screenshots of the application in action. These images demonstrate the question and answer interactions and the general user interface of the app.


## License

This project is licensed under the MIT License. See the [LICENSE](LICENSE) file for more details.
